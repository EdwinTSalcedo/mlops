{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYIGV2b7j6bc"
   },
   "source": [
    "# Despliegue\n",
    "\n",
    "En el presente notebook podras encontrar una implementación de TensorFlow para la clasificación de digitos manuscritos. Esta tiene como fin generar un modelo para su despliegue dentro de un contenedor docker. Esta vez, usaremos el dataset MNIST desde la librería TensorFlow Datasets, el cual es muy usado por la comunidad para primeros experimentos de Deep Learning y Machine Learning para reconocimiento de imágenes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iu2MPTdXjoAg",
    "outputId": "85e5a6e5-695a-4104-84e6-8de87bbec7f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edwinsalcedo/anaconda3/envs/mlops/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,log_loss, confusion_matrix, roc_curve, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 20:01:12.724275: W external/local_xla/xla/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Could not resolve hostname', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /Users/edwinsalcedo/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 100%|███████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.68 file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /Users/edwinsalcedo/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_set, dataset_info_train = tfds.load('mnist', split = 'train', as_supervised = True, with_info = True)\n",
    "testing_set, dataset_info_test = tfds.load('mnist', split = 'test', as_supervised = True, with_info = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existen 10 categorías en el dataset\n",
      "Existen 60000 imágenes en el conjunto de entrenamiento\n"
     ]
    }
   ],
   "source": [
    "# Ver caracteristicas del dataset\n",
    "num_classes = dataset_info_train.features['label'].num_classes\n",
    "print('Existen',num_classes,'categorías en el dataset')\n",
    "\n",
    "num_training_examples = dataset_info_train.splits['train'].num_examples\n",
    "print('Existen',num_training_examples, 'imágenes en el conjunto de entrenamiento')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 20:35:31.277721: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: <dtype: 'float32'>\n",
      "shape: (64, 28, 28, 1)\n",
      "Etiquetas: tf.Tensor(\n",
      "[2 5 9 9 8 5 6 6 2 7 9 7 8 9 6 9 5 8 0 3 1 8 1 8 6 6 8 3 6 2 8 7 8 6 5 2 5\n",
      " 7 1 2 3 7 0 2 0 6 2 0 5 7 5 5 1 5 4 6 0 2 2 4 6 2 3 7], shape=(64,), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 20:35:31.491309: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-05-20 20:35:31.493006: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-05-20 20:35:31.498710: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "def normalize(image, label):\n",
    "    # Convertir la imagen a float32 para usar valores decimales en el tensor\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # Dividir el tensor entre el nivel de intensidad mas alto en la imagen\n",
    "    image /= 255\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def prepare_subset(subset, batch_size):\n",
    "  # Almacenar el subconjunto de datos de entrenamiento en cache\n",
    "  batches = subset.cache()\n",
    "  # Barajar (intercambiar el orden) el subconjunto. Esta mezcla aleatoria se\n",
    "  # hara antes de procesar el dataset con el modelo. Podemos definir el comportamiento\n",
    "  # de esta función (cuan barajados estaran los lotes) definiendo un valor igual o\n",
    "  # menor al tamaño del subconjunto. Para realizar un barajamiento completo con\n",
    "  # respecto a todo el subconjunto, solo basta pasar como parametro el tamaño completo\n",
    "  # del subconjunto. En este caso tomaremos en cuenta a la cuarta parte.\n",
    "  batches = batches.shuffle(num_training_examples//4)\n",
    "  # Dividir al subconjunto en lotes y aplicarles la función normalize()\n",
    "  # prefetch(1) convierte a esta tarea en asincrona, por lo que seguira su ejecución\n",
    "  # sin paralizar toda la tarea de cargado de datos. De esta manera, las tareas de\n",
    "  # procesamiento de datos se llevaran a cabo en paralelo\n",
    "  batches = batches.batch(batch_size).map(normalize).prefetch(1)\n",
    "\n",
    "  return batches\n",
    "\n",
    "# Primer hiperparametro. Tamaño de lote.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "training_batches = prepare_subset(training_set, BATCH_SIZE)\n",
    "testing_batches = prepare_subset(testing_set, BATCH_SIZE)\n",
    "\n",
    "# Mostrar el contenido de un batch\n",
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    print('dtype:', image_batch.dtype)\n",
    "    print('shape:', image_batch.shape)\n",
    "    image_batch\n",
    "\n",
    "    print('Etiquetas:', label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edwinsalcedo/anaconda3/envs/mlops/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear red neuronal\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
    "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# Mostrar resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar los pesos y bias en el modelo\n",
    "model.build((None, 28, 28, 1))\n",
    "\n",
    "# Limpiar los datos en la RAM  que hubiesen sido guardados en entrenamientos anteriores\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zg2C01hokJVC",
    "outputId": "ab8ae5fc-605e-4d67-a9a6-4ba8f555049f"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "# Descargar dataset mnist\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1042 - loss: 2.2910 \n",
      "\n",
      "Perdida (Loss) antes del entrenamiento: 2.313\n",
      "Precisión antes del entrenamiento: 9.375%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 20:49:50.155544: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-05-20 20:49:50.157789: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-05-20 20:49:50.254432: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    loss, accuracy = model.evaluate(image_batch, label_batch)\n",
    "\n",
    "print('\\nPerdida (Loss) antes del entrenamiento: {:,.3f}'.format(loss))\n",
    "print('Precisión antes del entrenamiento: {:.3%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8542 - loss: 0.5014\n",
      "Epoch 2/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9632 - loss: 0.1232\n",
      "Epoch 3/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9751 - loss: 0.0808\n",
      "Epoch 4/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9814 - loss: 0.0617\n",
      "Epoch 5/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9856 - loss: 0.0467\n",
      "Epoch 6/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9888 - loss: 0.0364\n",
      "Epoch 7/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9899 - loss: 0.0318\n",
      "Epoch 8/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9921 - loss: 0.0252\n",
      "Epoch 9/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9927 - loss: 0.0219\n",
      "Epoch 10/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9939 - loss: 0.0186\n"
     ]
    }
   ],
   "source": [
    "# Entrenar modelo\n",
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit(training_batches, epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': 'auto', 'epochs': 10, 'steps': 938}\n",
      "[0.275433212518692, 0.11383441835641861, 0.07729622721672058, 0.05993761494755745, 0.04664028063416481, 0.03645765036344528, 0.029948456212878227, 0.02446831576526165, 0.02159309573471546, 0.018024666234850883]\n",
      "None\n",
      "[0.9197666645050049, 0.9659000039100647, 0.9760500192642212, 0.9815666675567627, 0.9855166673660278, 0.9881666898727417, 0.9906166791915894, 0.9919833540916443, 0.9927166700363159, 0.9939500093460083]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las métricas recolectadas durante cada época\n",
    "print(history.params)\n",
    "print(print(history.history['loss']))\n",
    "print(print(history.history['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x900 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Realizar inferencia con una instancia\n",
    "for image_batch, label_batch in testing_batches.take(1):\n",
    "  # Predecir resultados con un batch\n",
    "  ps = model.predict(image_batch)\n",
    "  # Recuperar la primera imagen\n",
    "  first_image = image_batch.numpy().squeeze()[0]\n",
    "\n",
    "# Visualizar la imagen y las probabilidades resultantes de pasar la imagen por el modelo\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "ax1.imshow(first_image, cmap = plt.cm.binary)\n",
    "ax1.axis('off')\n",
    "ax2.barh(np.arange(10), ps[0])\n",
    "ax2.set_aspect(0.1)\n",
    "ax2.set_yticks(np.arange(10))\n",
    "ax2.set_yticklabels(np.arange(10))\n",
    "ax2.set_title('Class Probability')\n",
    "ax2.set_xlim(0, 1.1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "5ISN-VE36g8N"
   },
   "outputs": [],
   "source": [
    "# Guardar el modelo entero como un SavedModel\n",
    "!mkdir -p saved_model\n",
    "model.save('saved_model/my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model.keras\n"
     ]
    }
   ],
   "source": [
    "!ls saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
